---
title: Perspectives on crowdsourcing annotations for natural language processing
authors:
- aobowang
- conghoang
- min
date: '2013-03-01'
publishDate: '2024-10-19T15:18:06.253195Z'
publication_types:
- article-journal
publication: '*Lang. Resour. Eval.*'
doi: 10.1007/s10579-012-9176-1
abstract: Crowdsourcing has emerged as a new method for obtaining annotations for
  training models for machine learning. While many variants of this process exist,
  they largely differ in their methods of motivating subjects to contribute and the
  scale of their applications. To date, there has yet to be a study that helps the
  practitioner to decide what form an annotation application should take to best reach
  its objectives within the constraints of a project. To fill this gap, we provide
  a faceted analysis of crowdsourcing from a practitioner's perspective, and show
  how our facets apply to existing published crowdsourced annotation applications.
  We then summarize how the major crowdsourcing genres fill different parts of this
  multi-dimensional space, which leads to our recommendations on the potential opportunities
  crowdsourcing offers to future annotation efforts.
tags:
- Wikipedia
- NLP
- Mechanical Turk
- Human computation
- Games with a purpose
- Crowdsourcing
- Annotation
links:
- name: URL
  url: https://doi.org/10.1007/s10579-012-9176-1
---
