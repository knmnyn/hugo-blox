@inproceedings{ramesh-kashyap-etal-2021-analyzing,
 abstract = {The robustness of pretrained language models(PLMs) is generally measured using performance drops on two or more domains. However, we do not yet understand the inherent robustness achieved by contributions from different layers of a PLM. We systematically analyze the robustness of these representations layer by layer from two perspectives. First, we measure the robustness of representations by using domain divergence between two domains. We find that i) Domain variance increases from the lower to the upper layers for vanilla PLMs; ii) Models continuously pretrained on domain-specific data (DAPT)(Gururangan et al., 2020) exhibit more variance than their pretrained PLM counterparts; and that iii) Distilled models (e.g., DistilBERT) also show greater domain variance. Second, we investigate the robustness of representations by analyzing the encoded syntactic and semantic information using diagnostic probes. We find that similar layers have similar amounts of linguistic information for data from an unseen domain.},
 address = {Kyiv, Ukraine},
 author = {Ramesh Kashyap, Abhinav  and
Mehnaz, Laiba  and
Malik, Bhavitvya  and
Waheed, Abdul  and
Hazarika, Devamanyu  and
Kan, Min-Yen  and
Shah, Rajiv Ratn},
 booktitle = {Proceedings of the Second Workshop on Domain Adaptation for NLP},
 editor = {Ben-David, Eyal  and
Cohen, Shay  and
McDonald, Ryan  and
Plank, Barbara  and
Reichart, Roi  and
Rotman, Guy  and
Ziser, Yftah},
 month = {April},
 pages = {222--244},
 publisher = {Association for Computational Linguistics},
 title = {Analyzing the Domain Robustness of Pretrained Language Models, Layer by Layer},
 url = {https://aclanthology.org/2021.adaptnlp-1.23},
 year = {2021}
}
