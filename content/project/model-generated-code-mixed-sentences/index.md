---
title: Model-generated Code-mixed Sentences

summary: We generate high-quality code-mixed sentences using language models.
abstract: "Code-switching/code-mixing is a common linguistic practice where speakers switch between multiple languages in a single discourse. Developing language models with code-switching capabilities is crucial to the interest of the large multilingual communities. However, collecting real-world code-mixed sentences for training remains challenging due to its colloquial nature, highlighting the importance of synthetic code-mixed data. Preliminary experiments shows LLMs can generate code-mixed sentences by switching entities in multiple languages. However, these model-generated sentences are often not natural limiting further leverage for training. This project explores methods to improve the naturalness of model-generated code-mixed sentences. Ultimately, we aim to build an automated pipeline capable of generating natural-sounding code-switched sentences for further downstream tasks."

tags: ["LLM", "Generation", "Multilingual NLP", "Code-mixing"]
year: 2025

date: '2025-01-01'

all_day: true

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides:

authors: ["barid", "min"]

---

Code-switching/code-mixing is a common linguistic practice where speakers switch between multiple languages in a single discourse. Developing language models with code-switching capabilities is crucial to the interest of the large multilingual communities. However, collecting real-world code-mixed sentences for training remains challenging due to its colloquial nature, highlighting the importance of synthetic code-mixed data. Preliminary experiments shows LLMs can generate code-mixed sentences by switching entities in multiple languages. However, these model-generated sentences are often not natural limiting further leverage for training. This project explores methods to improve the naturalness of model-generated code-mixed sentences. Ultimately, we aim to build an automated pipeline capable of generating natural-sounding code-switched sentences for further downstream tasks.
